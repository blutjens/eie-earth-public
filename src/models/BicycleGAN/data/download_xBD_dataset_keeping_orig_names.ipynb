{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Downloading xBD from the Google Bucket\n",
    "Link instance to bucket (https://cloud.google.com/compute/docs/disks/gcs-buckets#api)\n",
    "gcloud init # (exec in terminal; click 1 y 1)\n",
    "pip install --upgrade google-cloud-storage\n",
    "create account service credentials ( https://cloud.google.com/docs/authentication/production#cloud-console ) and copy them into the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<Bucket: eie_floods>, <Bucket: eie_internal_bigquery_repository>, <Bucket: eie_xview_processed>, <Bucket: eie_xview_raw>]\n",
      "<Bucket: eie_xview_raw>\n",
      "Downloading train set\n",
      "Downloading test set\n",
      "Downloading validation set\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from os.path import isfile, join\n",
    "import csv\n",
    "\n",
    "# Link account service credentils to this notebook\n",
    "def explicit():\n",
    "    from google.cloud import storage\n",
    "\n",
    "    # Explicitly use service account credentials by specifying the private key\n",
    "    # file.\n",
    "    storage_client = storage.Client.from_service_account_json(\n",
    "        'FDL US 2020 EIE-7be61e397b4f.json')\n",
    "\n",
    "    # Make an authenticated API request\n",
    "    buckets = list(storage_client.list_buckets())\n",
    "    print(buckets)\n",
    "explicit()\n",
    "\n",
    "from google.cloud import storage\n",
    "\n",
    "storage_client = storage.Client() \n",
    "bucket = storage_client.get_bucket(\"eie_xview_raw\") # Useful: Get names of buckets with storage_client.list_buckets()\n",
    "print(bucket)\n",
    "\n",
    "def download_all(prefix, dl_dir):\n",
    "    try:\n",
    "        original_umask = os.umask(0)\n",
    "        if not os.path.exists(dl_dir):\n",
    "            os.makedirs(dl_dir, mode=0o777)\n",
    "    finally:\n",
    "        os.umask(original_umask)\n",
    "#     allpaths = bucket.list_blobs(prefix=prefix)\n",
    "    blobs = bucket.list_blobs(prefix=prefix)  # Get list of files\n",
    "    \n",
    "#     f = open(dl_dir +'paths.csv', 'w')\n",
    "#     with f:\n",
    "#         writer = csv.writer(f)\n",
    "#         for row in [allpaths]:\n",
    "#             row = list(row)[21:-19]\n",
    "#             writer.writerow(row) #We save a .csv to link with google AI labeling tool to create segmentation masks later.\n",
    "    \n",
    "    #loops trough all files in the bucket under the prefix and downloads them into the dl_dir\n",
    "    for blob in blobs:\n",
    "        filename = blob.name.replace('/', '_')\n",
    "        blob.download_to_filename(dl_dir + filename)\n",
    "        \n",
    "for phase in ['train/', 'test/', 'val/']:        \n",
    "    try:\n",
    "        original_umask = os.umask(0)\n",
    "        dl_dir = './xBD/bigtiles/'+phase\n",
    "        if not os.path.exists(dl_dir):\n",
    "            os.makedirs(dl_dir, mode=0o777)\n",
    "    finally:\n",
    "        os.umask(original_umask)\n",
    "#Trainset\n",
    "print(\"Downloading train set\")\n",
    "prefix = 'training-set/train/images/hurricane'\n",
    "dl_dir = './xBD/bigtiles/train/'\n",
    "download_all(prefix, dl_dir)\n",
    "\n",
    "    \n",
    "#Testset\n",
    "print(\"Downloading test set\")\n",
    "prefix = 'test-set/test_images_labels_targets/test/images/hurricane'\n",
    "dl_dir = './xBD/bigtiles/test/'\n",
    "download_all(prefix, dl_dir)\n",
    "\n",
    "#validation\n",
    "print(\"Downloading validation set\")\n",
    "prefix = 'holdout-set/hold_images_labels_targets/hold/images/hurricane'\n",
    "dl_dir = './xBD/bigtiles/val/'\n",
    "download_all(prefix, dl_dir)\n",
    "print('Done')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Move post disaster into 'train_B' and pre disaster into 'train_A' (data file system used by Pix2pixHD).\n",
    "!cd xBD/bigtiles/ ; mkdir train_A train_B train_AB test_A test_B test_AB val_A val_B val_AB ; mv train/*post* train_B/ ; mv train/*pre* train_A/ ; mv test/*post* test_B/ ; mv test/*pre* test_A/ ; mv val/*post* val_B/ ; mv val/*pre* val_A/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create aligned (concatenated) side-by-side .png files for the trainset of bicycleGAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import os\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from os.path import isfile, join\n",
    "import csv\n",
    "\n",
    "def A_and_B_to_AB(path=\"./xBD/bigtiles/\"):\n",
    "    print('Make sure files are named in a way that sorting them alphabetically \\\n",
    "        both in *_A and *_B folders produces the same order (i.e., ideally, \\\n",
    "        files are named equally but placed in different *_A and *_B folders')\n",
    "        \n",
    "    for folder in ['train','test','val']:\n",
    "        A_dir = path + folder + '_A/'\n",
    "        B_dir = path + folder + '_B/'\n",
    "        AB_dir = path + folder + '_AB/'\n",
    "        print('Generating ',AB_dir)\n",
    "        try:\n",
    "            os.mkdir(AB_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        train_A_list = [f.strip() for f in os.listdir(A_dir) if isfile(join(A_dir, f)) and valid_extension(join(A_dir, f))]\n",
    "        train_A_list.sort()\n",
    "\n",
    "        train_B_list = [f.strip() for f in os.listdir(B_dir) if isfile(join(B_dir, f)) and valid_extension(join(B_dir, f))]\n",
    "        train_B_list.sort()\n",
    "\n",
    "        for img_path_A, img_path_B in zip(train_A_list, train_B_list):\n",
    "            # assert img_path_A == img_path_B, 'Make sure files are named in a \\\n",
    "            # way that sorting them alphabetically both in *_A and *_B folders \\\n",
    "            # produces the same order (i.e., ideally, files are named equally but \\\n",
    "            # placed in different *_A and *_B folders: {}\\n{}'.format(img_path_A, img_path_B)\n",
    "            print('Generating {} image AB for {}'.format(folder,img_path_A))\n",
    "            images = [Image.open(x) for x in [A_dir + img_path_A, B_dir+img_path_B]]\n",
    "            widths, heights = zip(*(i.size for i in images))\n",
    "\n",
    "            total_width = sum(widths)\n",
    "            max_height = max(heights)\n",
    "\n",
    "            new_im = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "            x_offset = 0\n",
    "            for im in images:\n",
    "                new_im.paste(im, (x_offset,0))\n",
    "                x_offset += im.size[0]\n",
    "\n",
    "            new_im.save(path+folder+'_AB/'+img_path_A)\n",
    "        print('Done with ' +folder)\n",
    "    \n",
    "        \n",
    "A_and_B_to_AB(\"./xBD/bigtiles/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create 256x256 dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bigtiles_to_smalltiles(bigtiles=\"./xBD/bigtiles/\", small_path=\"./xBD/smalltiles/\",output_size=256):\n",
    "    folders = ['train_A', 'train_B', 'train_AB', 'test_A', 'test_B', 'test_AB', 'val_A', 'val_B', 'val_AB']\n",
    "    for folder in folders:\n",
    "        directory = small_path+folder\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory, mode=0o777)\n",
    "        try:\n",
    "            original_umask = os.umask(0)\n",
    "        finally:\n",
    "            os.umask(original_umask)\n",
    "        \n",
    "    def crop_tiles(image):\n",
    "        image = np.array(image)\n",
    "        width = image.shape[0]\n",
    "        N = width/output_size\n",
    "        h_tiles = np.split(image, N)\n",
    "        \n",
    "        images = []\n",
    "        for tile in h_tiles:\n",
    "            images.append(np.split(tile, N, axis=1))\n",
    "        images = [item for sublist in images for item in sublist] #flatten nested list\n",
    "        return images\n",
    "    \n",
    "    def load_crop_save(list_files, path):\n",
    "        for i in range(len(list_files)):\n",
    "            image = Image.open(\"./xBD/bigtiles/\"+path+list_files[i])\n",
    "            images = crop_tiles(image)\n",
    "            for num , im in enumerate(images): # ToDo, not tested but should be not num but str(im) as otherwise later we require in *_AB\n",
    "                # toDo: rm 0*.png;rm 1*.png; rm 2*.png;rm  3*.png; rm 4*.png;rm 5*.png;rm 6*.png; rm 7*.png ; rm 8*.png; rm 9*.png; \n",
    "                name = list_files[i][:-4]+str(im).zfill(2)+'.png'\n",
    "                new_im = Image.fromarray(im)\n",
    "                new_im.save('./xBD/smalltiles/'+path+name)\n",
    "    \n",
    "    for dataset in ['train','test','val']:\n",
    "        A_dir = bigtiles + dataset + '_A/'\n",
    "        B_dir = bigtiles + dataset + '_B/'\n",
    "\n",
    "        train_A_list = [f for f in os.listdir(A_dir) if isfile(join(A_dir, f))]\n",
    "        train_A_list.sort()\n",
    "        print(\"Cropping \"+ dataset+ \" A\")\n",
    "        load_crop_save(train_A_list, dataset+'_A/')\n",
    "\n",
    "        train_B_list = [f for f in os.listdir(B_dir) if isfile(join(B_dir, f))]\n",
    "        train_B_list.sort()\n",
    "        print(\"Cropping \"+dataset+\" B\")\n",
    "        load_crop_save(train_B_list, dataset+'_B/')\n",
    "    print('Done with '+dataset)\n",
    "\n",
    "\n",
    "def extension(filename):\n",
    "    return filename.split('.')[-1]\n",
    "\n",
    "def valid_extension(filename):\n",
    "    return extension(filename) in set(['jpg','png','jpeg'])\n",
    "\n",
    "def A_and_B_to_AB(path=\"./xBD/bigtiles/\"):\n",
    "    print('Make sure files are named in a way that sorting them alphabetically \\\n",
    "        both in *_A and *_B folders produces the same order (i.e., ideally, \\\n",
    "        files are named equally but placed in different *_A and *_B folders')\n",
    "        \n",
    "    for folder in ['train','test','val']:\n",
    "        A_dir = path + folder + '_A/'\n",
    "        B_dir = path + folder + '_B/'\n",
    "        AB_dir = path + folder + '_AB/'\n",
    "        print('Generating ',AB_dir)\n",
    "        try:\n",
    "            os.mkdir(AB_dir)\n",
    "        except FileExistsError:\n",
    "            pass\n",
    "        train_A_list = [f.strip() for f in os.listdir(A_dir) if isfile(join(A_dir, f)) and valid_extension(join(A_dir, f))]\n",
    "        train_A_list.sort()\n",
    "\n",
    "        train_B_list = [f.strip() for f in os.listdir(B_dir) if isfile(join(B_dir, f)) and valid_extension(join(B_dir, f))]\n",
    "        train_B_list.sort()\n",
    "\n",
    "        for img_path_A, img_path_B in zip(train_A_list, train_B_list):\n",
    "            # assert img_path_A == img_path_B, 'Make sure files are named in a \\\n",
    "            # way that sorting them alphabetically both in *_A and *_B folders \\\n",
    "            # produces the same order (i.e., ideally, files are named equally but \\\n",
    "            # placed in different *_A and *_B folders: {}\\n{}'.format(img_path_A, img_path_B)\n",
    "            print('Generating {} image AB for {}'.format(folder,img_path_A))\n",
    "            images = [Image.open(x) for x in [A_dir + img_path_A, B_dir+img_path_B]]\n",
    "            widths, heights = zip(*(i.size for i in images))\n",
    "\n",
    "            total_width = sum(widths)\n",
    "            max_height = max(heights)\n",
    "\n",
    "            new_im = Image.new('RGB', (total_width, max_height))\n",
    "\n",
    "            x_offset = 0\n",
    "            for im in images:\n",
    "                new_im.paste(im, (x_offset,0))\n",
    "                x_offset += im.size[0]\n",
    "\n",
    "            new_im.save(path+folder+'_AB/'+img_path_A)\n",
    "        print('Done with ' +folder)\n",
    "    \n",
    "bigtiles_to_smalltiles()\n",
    "\n",
    "A_and_B_to_AB(\"./xBD/bigtiles/\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "environment": {
   "name": "pytorch-gpu.1-4.m50",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/pytorch-gpu.1-4:m50"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
